{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary evaluation\n",
    "\n",
    "Today we'll take a look at how we can evaluate the quality of model-generated summaries in different ways.\n",
    "\n",
    "## Install packages\n",
    "\n",
    "Tip: You might need to restart the jupyter kernel after installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rouge_score\n",
    "%pip install bert_score \n",
    "%pip install blanc \n",
    "%pip install nltk \n",
    "%pip install sentencepiece \n",
    "%pip install protobuf \n",
    "%pip install transformers \n",
    "%pip install datasets \n",
    "%pip install spacy\n",
    "%pip install evaluate\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We'll use a small slice of the English part of the `xlsum` dataset from the `datasets` library. You can take a look at what kind of data this includes [here](https://huggingface.co/datasets/csebuetnlp/xlsum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"csebuetnlp/xlsum\", \"english\", split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles are in the `text` column and the summaries are in the `summary` column. Let's extract them and take a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ds[\"text\"][0:10]\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = ds[\"summary\"][0:10]\n",
    "reference_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss:\n",
    "- Based on these examples, what do you think of the quality of the dataset?\n",
    "- Do you foresee any potential pitfalls for evaluation, based on your observations?\n",
    "\n",
    "Let's take a look into the density of the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.fragments import Fragments\n",
    "\n",
    "fragment = [Fragments(summary, article, lang=\"en\") for summary, article in zip(reference_summaries, articles)]\n",
    "density = [frag.density() for frag in fragment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(filter(lambda x: x <= 1.5, density))) / len(density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember, summaries with density values below 1.5 are considered abstractive, meaning these seem to be highly abstractive summaries.\n",
    "However, the density values are not a perfect measure of abstractive quality:\n",
    "- Can you think of a way we might be able to \"game\" the density metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating summaries\n",
    "Now let's generate some summaries using a pre-trained model. We'll use the `mt5-small` model from the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/mt5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, min_length=10, max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make everything a bit easier for ourselves, let's make a function which:\n",
    "1. Takes an input text\n",
    "2. Tokenises the text (remember to set the padding and truncation arguments to True)\n",
    "3. Generates a summary based on the tokenised input (and prompt, if you're so inclined)\n",
    "4. Decodes the generated summary from tokens into words, and\n",
    "5. Returns the output\n",
    "\n",
    "(Hint: there is one potential solution in the class_8_solution notebook, if you're in need :-))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use that function to generate some summaries for the articles in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_pipeline_function(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summaries = [your_pipeline_function(article) for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now let's evaluate the quality of the generated summaries with some commonly used metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "rouge.compute(references=reference_summaries, predictions=generated_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the ROUGE scores for the individual summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge.compute(references=reference_summaries, predictions=generated_summaries, use_aggregator=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERTScore metric does not use an aggregator, but we can average the scores ourselves to get an overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "bertscores = bertscore.compute(references=reference_summaries, predictions=generated_summaries, lang=\"en\")\n",
    "bertscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(bertscores[\"precision\"]), np.mean(bertscores[\"recall\"]), np.mean(bertscores[\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try a reference-free metric, such as BLANC, in case we do not have access to reference summaries, or we do not want to rely on them due to quality, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blanc\n",
    "\n",
    "blanc = blanc.BlancHelp()\n",
    "blanc.eval_pairs(articles, generated_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss:\n",
    "- What do these values tell us about the quality of the generated summaries?\n",
    "- What are the strenghts and weaknesses of using reference-free metrics?\n",
    "- What are the potential weaknesses of using a less known metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now, the summaries we generated aren't exactly great, likely because the mt5 model was not fine-tuned for that purpose.\n",
    "- Try to generate 10 new summaries using a model that has been fine-tuned for summarisation (e.g., our old friend, flan-t5-small)\n",
    "- When you have the summaries, evaluate them using the same quantitative metrics as before\n",
    "- Then try to conduct a qualitative evaluation of the summaries - in your groups, decide on some evalaution criteria (e.g., ranking, \"stars\", etc.), evaluate the summaries based on these criteria, and compare your results within the group and with the quantitative metrics\n",
    "\n",
    "### Bonus exercise\n",
    "Try to create a LLM judge that can evaluate the quality of the summaries based on the criteria you defined.\n",
    "- Load in a generative pre-trained model from huggingface\n",
    "- Prompt it with your evaluation criteria\n",
    "- Compare its evaluation with your own"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
