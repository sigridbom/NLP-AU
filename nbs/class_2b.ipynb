{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install asent\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asent import lexicons\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data\n",
    "\n",
    "The data is loaded using the [```asent```](\"https://github.com/KennethEnevoldsen/asent\") package. It contains 7504 words and a continuous sentiment label which is constructed by [two annotaters](\"https://ojs.aaai.org/index.php/ICWSM/article/view/14550\").\n",
    "\n",
    "I have split the data into a training and a test set containing 80% and 20% of the data, respectively. It is important that we all use the same random state, to ensure we get the same split. That way, we can compare our models on the held-out test set next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = pd.DataFrame(lexicons.get(\"lexicon_en_v1\").items(), columns=[\"word\", \"sentiment\"])\n",
    "\n",
    "train, test = train_test_split(lex, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and feature generation\n",
    "\n",
    "To make the task a bit simpler, we binarise the sentiment label. We consider all words with a sentiment score above 0 as positive (1) and all words with a sentiment score below 0 as negative (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1 if x>0 else 0 for x in train[\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"sentiment\"].to_list())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our labels for training, and we can start selecting what features to use to predict the sentiment of the words. I'm going to use the ```glove``` embeddings from earlier, but feel free to use any other embeddings or features you think will work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words are not in the embedding model vocabulary, so we need to decide how to represent them instead. I'm going to use a zero vector, but another common approach is to use the average of the embeddings of all other words (mean imputation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [embeddings[r[\"word\"]] if r[\"word\"] in embeddings.index_to_key else np.zeros(shape=300) for i, r in train.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to transform the features to a format that the classifier can use. I'm going to use a simple logistic regression model from ```sklearn```, but you can use any other classifier you think will work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have specified your model, you can fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class has a ```score()``` method that takes features and true labels and returns an accuracy score. It uses the fitted model to predict labels based on the features and compares them to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a sentiment classifier! If there indeed is a strong relationship between the sentiment of a word and its embedding, the relationships learned during training should generalise to the test set, which we will test next week.\n",
    "\n",
    "For now, you can iterately improve the model by tweaking different parts of the pipeline, and reevaluating the performance on the training set. Here are some parameters you can try to change:\n",
    "\n",
    "- try embedding models trained on different data (e.g., you can get an overview of different embeddings models that are available through the gensim api by running ```gensim.downloader.info()[\"models\"].keys()```)\n",
    "- try mean imputation\n",
    "- try to change the parameters of the model (you can find an overview of the parameters in the [sklearn documentation](\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\"))\n",
    "- try different models (you can find an overview of the sklearn supervised learning library [here](\"https://scikit-learn.org/stable/supervised_learning.html\"))\n",
    "- try to use the continuous sentiment scores as labels - can you get similar performance? (why/why not?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
