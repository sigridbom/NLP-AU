{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install accelerate\n",
    "!pip install pyarrow\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we are breaking up the pipeline function from transformers that we have used previously. One of things the pipeline was doing behind the curtain was tokenising the text, but we can just as easily do that in a separate step.\n",
    "\n",
    "Huggingface lets us initialize our model and tokenizer with the .from_pretrained() method, which will ensure that:\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use, and\n",
    "- we download the vocabulary used when pretraining this specific checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, max_length=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's try to tokenise some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"My name is \"\n",
    "\n",
    "tokenized_text = tokenizer(input_text, return_tensors=\"pt\")\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a dictionary, the first part of which are the input_ids are the IDs of the tokens in the vocabulary. We can check this by decoding the IDs back into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([564])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part is the attention mask, which is a binary mask that tells the model which tokens to pay attention to and which to ignore (remember the causal vs fully-visible attention mask?). This is useful when we have padded our input to be the same length as the longest sequence in the batch, and we want the model to ignore the padding tokens.\n",
    "\n",
    "Batched inputs are often different lengths, so they canâ€™t be converted to fixed-size tensors. Padding and truncation are strategies for dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a special padding token to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model.\n",
    "\n",
    "Try to add padding=\"max_length\" to the tokenizer and see what happens. (You can find all the different possible values for the argument in the documentation: https://huggingface.co/transformers/main_classes/tokenizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncation works in the other direction by truncating long sequences to the maximum length the model can accept. Try to insert a long sentence and add the truncation argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, padding your batch to the length of the longest sequence and truncating to the maximum length a model can accept works pretty well.\n",
    "\n",
    "Now, let's move on to another part of the pipeline, which corresponds to the .generate() method. This method takes the token IDs and generates the next token IDs. We can do this in a separate step as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(tokenized_text[\"input_ids\"])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then only need to decode the IDs back into words to get the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try and use the GPU for this task! We can do this by moving the model and the inputs to the GPU using the .to() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "model.generate(tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the device used is now cuda (the GPU) and the processing time is way faster!\n",
    "\n",
    "Task\n",
    "- Make your own function that works like the pipeline, but using the tokenization and generation steps we just saw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine translation\n",
    "\n",
    "This week, we will attempt machine translation.\n",
    "\n",
    "The dataset is the [OPUS-100](https://huggingface.co/datasets/Helsinki-NLP/opus-100) which contains translation pairs from over 100 languages. I chose the Danish and English translation pairs because that makes it easier for me to evaluate the quality of the translations, but feel very free to choose a different language pair if you prefer. You can see the different language pairs available in the \"Subset\" part of the dataset viewer in the link above.\n",
    "\n",
    "We'll use huggingface's datasets library to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/opus-100\", \"da-en\", split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translation pairs are nested in the translation column, so we need to flatten the dataset to get the source and target language in separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_cols(row):\n",
    "    row[\"en\"] = row[\"translation\"][\"en\"]\n",
    "    row[\"da\"] = row[\"translation\"][\"da\"]\n",
    "    return row\n",
    "\n",
    "train = ds.map(unpack_cols, remove_columns=[\"translation\"])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to pick a few sentences and see how well the model translates them without any additional help - just giving the source language sentence as input and letting the model generate the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to pick a few sentences and see how well the model can translate out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_pipeline_function(train[150]['en'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did it work? If not, why?\n",
    "\n",
    "If zero shot prompting didn't work,\n",
    "\n",
    "- try one shot and\n",
    "- few shot prompting, to see if providing context helps the model to generate better translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try instruction  tuning the model to hopefully get a better result!\n",
    "\n",
    "The datasets library has a nice map method that we can use to apply a function to all the examples in the dataset. The map method can take a custom function, so we just need to write a function that prepares our data for the model.\n",
    "\n",
    "Write preprocessing function that takes in a batch of the dataset and\n",
    "- defines an instruction and appends it to the input text\n",
    "- creates a list of all input texts (hint: you can use a loop or list comprehension)\n",
    "- creates a new column in the dataset called \"input_ids\" that contains the token ids of the input text (hint: you can use the tokenizer on the list of input texts)\n",
    "- creates a list of all output texts\n",
    "- creates a new column in the dataset called \"labels\" that contains the token ids of the output text\n",
    "- returns the batch\n",
    "\n",
    "If you need a bit of help, I've started the first three steps for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_func(batch):\n",
    "    instruction = \n",
    "    input_texts = [instruction + row + for row in batch['en']]\n",
    "    batch[\"input_ids\"] = tokenizer(input_texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(\"cuda\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = train.map(preprocessing_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_train.remove_columns([\"en\", \"da\"])\n",
    "tokenized_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready for the model, so we can fine-tune it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to initalize a Trainer class.\n",
    "\n",
    "To do this, we have to defined the TrainingArguments, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional.\n",
    "\n",
    "I have changed a few parameters, like the learning rate and weight decay, as well as setting the max number of steps (so it doesn't run for a very long time) and the logging steps (so we get updated more frequently on the loss) and the batch size (also for speed). If you want to play around with changing other parameters, you can find the full list of arguments in the documentation (https://huggingface.co/docs/transformers/en/main_classes/trainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(output_dir=\"./flan-t5-small-da-en\",\n",
    "   per_device_train_batch_size=4,\n",
    "   learning_rate=1e-3,\n",
    "   weight_decay=0.01,\n",
    "   max_steps=3000,\n",
    "   logging_steps=200,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train! Buckle up, this will probably take a bit of time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model and load it in as a pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"instruct-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"instruct-model\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust your pipeline to incorporate the new instruct_model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_pipeline_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to test the finetuned model on the examples from before.\n",
    "\n",
    "- Does it perform better than before? Why/why not?\n",
    "- What happens if you instruct the model to perform a different task, like summarisation or reasoning? Does the performance gain transfer? Why/why not?\n",
    "- If you wanted to instruction tune a model to be able to solve a wide variety of tasks (like chatpgt), what kind of training data would you need?\n",
    "- How would you produce that kind of data and what are the possible limitations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
