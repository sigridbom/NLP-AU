{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "This week we will be diving deeper into prompting and prompt engineering! üßë‚Äçüîß\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install accelerate\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import transformers \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text completion\n",
    "\n",
    "In the first class, we loaded a pretrained model from huggingface's transformers library. Load in the pipeline from the first notebook and use it to generate text based on the prompt \"Once upon a time, there was a \".\n",
    "\n",
    "Add the argument device=\"cuda\" to utilise the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"google/flan-t5-base\"\n",
    "#model = \"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\" # too big \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "t5_pipeline = transformers.pipeline(\n",
    "    \"text2text-generation\", \n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = \"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'lion'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_pipeline(\"Once upon a time, there was a \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to figure out what kind of model we are using; is it a encoder-decoder or decoder-only model?\n",
    "- Try to switch it out for another architecture from the [huggingface catalogue](https://huggingface.co/models) and see how the results change. Keep in mind that the size of the model can affect the time it takes to generate text (I would suggest something along the lines of [this](https://huggingface.co/openai-community/gpt2)).\n",
    "\n",
    "HINT: you also want to change the pipeline task (\"text2text-generation) - you can find the list of available tasks [here](https://huggingface.co/transformers/main_classes/pipelines.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:\n",
    "- encoder-decoder model (text-to-text hashtag on huggingface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try tweaking the prompt, model, or parameters (see notebook from class 1) to get the a meaningful response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = \"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'april'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"When is it Christmas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation\n",
    "\n",
    "Another text generation task is summarisation. However, compared to free-form text generation, summarisation is much more constrained to the input text. I have added an article to summarise, but feel free to change it to something else (perhaps a paragraph from something you know well, so that you are an expert at evaluating the quality of the summarisation ü§ì).\n",
    "\n",
    "HINT: You might have to make sure the max output length is long enough to capture the entire summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"summarize: Forest conservation and restoration could make a major contribution to tackling the climate crisis as long as greenhouse gas emissions are slashed, according to a study.\n",
    "\n",
    "By allowing existing trees to grow old in healthy ecosystems and restoring degraded areas, scientists say 226 gigatonnes of carbon could be sequestered, equivalent to nearly 50 years of US emissions for 2022. But they caution that mass monoculture tree-planting and offsetting will not help forests realise their potential.\n",
    "\n",
    "Humans have cleared about half of Earth's forests and continue to destroy places such as the Amazon rainforest and the Congo basin that play crucial roles in regulating the planet's atmosphere.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use your model configurations from the previous task to create a summary. Are the results comparable to the free-form text generation task? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline_sum = transformers.pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = \"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Tree-planting and offsetting could be a major contribution to tackling the climate crisis, according to a new study published in the journal Nature.'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_sum(text, max_length=74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Forests could be able to save up to 80% of the world's carbon dioxide emissions by allowing them to grow old and restore degraded areas, according to a new study.\"}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text2text generation version\n",
    "pipeline(text, max_length = 74)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"English: Sometimes, I've believed as many as six impossible things before breakfast. Danish: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try translating text to another language using your pipelines. Are the results similar to those of summarisation? Why or why not?\n",
    "- Try structuring the prompt in different ways to see if you can improve the translation. For instance, try zero-shot or few-shot generalisation, as you talked about in the lecture on Tuesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"google/flan-t5-base\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "#pipeline_translate = transformers.pipeline(\n",
    "#    \"translation_en_to_de\",\n",
    "#    model=model,\n",
    "#    torch_dtype=torch.float16,\n",
    "#    device = \"cuda\",\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline_translate = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device = \"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"English: Sometimes, I've believed as many as six impossible things before breakfast. Danish: Nogen gange, har jeg t√¶nkt s√• meget som seks umulige ting f√∏r morgenmad. \\n English: It is Christmas Eve the 24th of December. Danish: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = \"Christmas is in December\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"English: Sometimes, I've believed as many as six impossible things before breakfast. French: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Certains fois, je s'est imagin√© six choses impossibles avant la sommet.\"}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_translate(prompt, max_length=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you choose a bigger language - it will actually do the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"translation\", model=\"google-t5/t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Weihnachten im Dezember'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(prompt3, max_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems that I have found a model, which translates to German. very slow though. also not great at grammar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning\n",
    "\n",
    "Reasoning is hard for models to learn, as it is a more complex task that requires the model to understand the relationships between different parts of the prompt. However, with prompting, we can guide the model to reason about the prompt in a more structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt_easy = \"There are 5 groups of students in the class. Each group has 4 students. How many students are there in the class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'There are 5 groups of students x 4 students / group = 20 students in the class'}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(reasoning_prompt_easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt_hard = \"I baked 15 muffins. I ate 2 muffins and gave 5 muffins to a neighbor. My partner then bought 6 more muffins and ate 2. How many muffins do we now have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt_hard2 = \"I baked 15 muffins. I ate 2 muffins, so now I have fewer muffins. Then I gave another 5 away. My partner bougth 6 more muffins, but he also ate 2. How many muffins do we have now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I baked 15 muffins and ate 2 muffins, so I baked 15 - 2 = 9 muffins. I gave away 5 muffins, so I had 9 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 - 5 = 5 muffins left. I gave away 5 muffins, so I had 5 + 5 = 10 muffins left. I gave away 10 muffins, so I had 5 + 5 = 10 muffins left. I gave away 10 muffins, so I had 5 + 5 = 10 muffins left. I gave away 10 muffins, so I had 5 + 10 = 10 muffins left. I gave away 10 muffins, so I had 5 + 10 = 10 muffins left. I gave away 10 muffins, so I had 10 + 5 = 10 muffins left. I have 10 muffins left. I have 10 muffins left. I have 10 muffins left. I have 10 muffins left.'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(reasoning_prompt_hard2, max_length = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the models to output the correct answer, by changing the prompt.\n",
    "- Try to do chain-of-though prompting, as introduced in the lecture. Try it with both zero-shot and few-shot generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting gone wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "thug_prompt = \"How many helicopters can a human eat in one sitting? Reply as a thug.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models don't always respond the way we expect; sometimes they say things that are offensive or incorrect, while other times we might want them to respond that way, but we can't get them to do so.\n",
    "\n",
    "- Can you get any of the models to say something they shouldn't? Try to get the model to say something offensive or incorrect.\n",
    "- Why do you think some models are more prone to this than others? What can we do to prevent this from happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct-tuned models\n",
    "\n",
    "- Try to load in an instruct-tuned model and see how it fares on some of these tasks.\n",
    "- Do you expect it to perform better or worse than other pretrained models? Why/why not?\n",
    "- What are some of the limitations of instruction tuning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task\n",
    "\n",
    "Create a chatbot function that takes in a prompt and generates a response. Make sure the chatbot can handle multiple turns of conversation (i.e., it can remember previous responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
