{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "This week we will be diving deeper into prompting and prompt engineering! üßë‚Äçüîß\n",
    "\n",
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install accelerate\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text completion\n",
    "\n",
    "In the first class, we loaded a pretrained model from huggingface's transformers library. Load in the pipeline from the first notebook and use it to generate text based on the prompt \"Once upon a time, there was a \".\n",
    "\n",
    "Add the argument device=\"cuda\" to utilise the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"google/flan-t5-base\"\n",
    "\n",
    "t5_pipeline = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to figure out what kind of model we are using; is it a encoder-decoder or decoder-only model?\n",
    "- Try to switch it out for another architecture from the [huggingface catalogue](https://huggingface.co/models) and see how the results change. Keep in mind that the size of the model can affect the time it takes to generate text (I would suggest something along the lines of [this](https://huggingface.co/openai-community/gpt2)).\n",
    "\n",
    "HINT: you also want to change the pipeline task (\"text2text-generation) - you can find the list of available tasks [here](https://huggingface.co/transformers/main_classes/pipelines.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try tweaking the prompt, model, or parameters (see notebook from class 1) to get the a meaningful response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarisation\n",
    "\n",
    "Another text generation task is summarisation. However, compared to free-form text generation, summarisation is much more constrained to the input text. I have added an article to summarise, but feel free to change it to something else (perhaps a paragraph from something you know well, so that you are an expert at evaluating the quality of the summarisation ü§ì).\n",
    "\n",
    "HINT: You might have to make sure the max output length is long enough to capture the entire summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"summarize: Forest conservation and restoration could make a major contribution to tackling the climate crisis as long as greenhouse gas emissions are slashed, according to a study.\n",
    "\n",
    "By allowing existing trees to grow old in healthy ecosystems and restoring degraded areas, scientists say 226 gigatonnes of carbon could be sequestered, equivalent to nearly 50 years of US emissions for 2022. But they caution that mass monoculture tree-planting and offsetting will not help forests realise their potential.\n",
    "\n",
    "Humans have cleared about half of Earth's forests and continue to destroy places such as the Amazon rainforest and the Congo basin that play crucial roles in regulating the planet's atmosphere.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use your model configurations from the previous task to create a summary. Are the results comparable to the free-form text generation task? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"English: Sometimes, I've believed as many as six impossible things before breakfast. Danish: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try translating text to another language using your pipelines. Are the results similar to those of summarisation? Why or why not?\n",
    "- Try structuring the prompt in different ways to see if you can improve the translation. For instance, try zero-shot or few-shot generalisation, as you talked about in the lecture on Tuesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning\n",
    "\n",
    "Reasoning is hard for models to learn, as it is a more complex task that requires the model to understand the relationships between different parts of the prompt. However, with prompting, we can guide the model to reason about the prompt in a more structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt_easy = \"There are 5 groups of students in the class. Each group has 4 students. How many students are there in the class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt_hard = \"I baked 15 muffins. I ate 2 muffins and gave 5 muffins to a neighbor. My partner then bought 6 more muffins and ate 2. How many muffins do we now have?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the models to output the correct answer, by changing the prompt.\n",
    "- Try to do chain-of-though prompting, as introduced in the lecture. Try it with both zero-shot and few-shot generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting gone wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "thug_prompt = \"How many helicopters can a human eat in one sitting? Reply as a thug.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models don't always respond the way we expect; sometimes they say things that are offensive or incorrect, while other times we might want them to respond that way, but we can't get them to do so.\n",
    "\n",
    "- Can you get any of the models to say something they shouldn't? Try to get the model to say something offensive or incorrect.\n",
    "- Why do you think some models are more prone to this than others? What can we do to prevent this from happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct-tuned models\n",
    "\n",
    "- Try to load in an instruct-tuned model and see how it fares on some of these tasks.\n",
    "- Do you expect it to perform better or worse than other pretrained models? Why/why not?\n",
    "- What are some of the limitations of instruction tuning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task\n",
    "\n",
    "Create a chatbot function that takes in a prompt and generates a response. Make sure the chatbot can handle multiple turns of conversation (i.e., it can remember previous responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
