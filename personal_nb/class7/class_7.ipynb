{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning & a bit of Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "# !pip install torch \"transformers[torch]\" datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: architecture of a Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what do we have here?\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of some of the dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens in the vocabulary\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum length of input (in tokens)\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensionality of representations (in neurons)\n",
    "model.config.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensionality of the hidden layer(s) (in neurons) \n",
    "# that come after the transformer blocks\n",
    "model.config.hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134734080"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of trainable parameters\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:  \n",
    "Is 134M parameters a lot?  \n",
    "Compared to current state-of-art models, and compared to the original BERT model?\n",
    "\n",
    "**If you want to find out at home**:   \n",
    "How did we get to this number of parameters?  \n",
    "In other words, which hyperparameters do we have to add and multiply to get to this number?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going fine-tune and probe on our use case today: \n",
    "## Named Entity Recognition\n",
    "\n",
    "Brief intro:  \n",
    "Named entity recognition (NER) aims to find tokens in an unstructured text that contain some real world object, such as a person's name, a location, an organization, time, currency and so on. Often, NER is the task of finding Proper Nouns. \n",
    "\n",
    "A popular problem in NER is the [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) task, which uses the categories, person (PER), organization (ORG) location (LOC) and miscellaneous (MISC). For example:\n",
    "\n",
    "*$Franz$ $Kafka_{PER}$ studied at $Charles$ $University_{ORG}$ in $Prague_{LOC}$*\n",
    "\n",
    "Let's get the corrected version CoNLL-2003:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\n",
    "    \"conllpp\",\n",
    "    trust_remote_code=True #this time we'll trust remote code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note, this is a DatasetDict\n",
    "# which consists of three Datasets\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one datapoint looks like this\n",
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER tags are displayed as numbers when showing a single data point, but behind the scenes, it is a `ClassLabel`.   \n",
    "This means there is a mapping of integers to label name. `0` corresponds to `O`, and `1` corresponds to `B-PER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to understand these labels?\n",
    "\n",
    "There are different tag standards for NER.  \n",
    "Probably the most used one is the IOB-format which frames the task as token classification denoting the **B**eginning, **I**nside, or **O**utside of a named entity.\n",
    "\n",
    "- Words marked with O are not a named entity.  \n",
    "- B-* indicate a named entity (e.g. Aarhus), or the start of a multiword entity (i.e. B-ORG for the Aarhus in Aarhus University)\n",
    "- I-* indicate the continuation of a token (e.g. University)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ds[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary to translate integers to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-ORG',\n",
       " 5: 'B-LOC',\n",
       " 6: 'I-LOC',\n",
       " 7: 'B-MISC',\n",
       " 8: 'I-MISC'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {i: lab for i, lab in enumerate(label_list)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we'll initialize a new model for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# NB: we're changing the model to a token classification one\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "# and adding a label dictionary\n",
    "model.config.id2label = id2label\n",
    "\n",
    "# why AutoModelForTokenClassification?\n",
    "# what different kinds of auto-models there are – link for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForTokenClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model looks different now\n",
    "# notice the last two parts that were added\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'B-MISC',\n",
       " 'B-MISC',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'I-ORG',\n",
       " 'B-MISC']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    \"Franz Kafka studied at Charles University in Prague\", return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# which index is the maximum of the last label?\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "predicted_tokens_classes = [id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is complete nonsense. That is why we need:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Remember the Semantic Textual Similarity experiment from class 5?  \n",
    "That was a case of zero-shot learning: we tested the capabilities of the model without preparing it for the task.\n",
    "\n",
    "This time we want to teach the model to solve a concrete task.   \n",
    "Much of this is adapted from Huggingface's documentation.  \n",
    "[Here](https://github.com/huggingface/transformers/tree/main/notebooks) you will find tutorials on how to fine-tune for many different tasks.\n",
    "\n",
    "There is an important pre-processing step at the start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 're',\n",
       " '##jects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boy',\n",
       " '##cott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds[\"train\"][0]\n",
    "\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two great inventions of BERT-style tokenization just turned into two problems:\n",
    "- Subword tokenization can split one word into multiple tokens. The tokens no longer align with the NER labels.\n",
    "- We now have special tokens! They don't have NER labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem\n",
    "len(tokens) == len(example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to:\n",
    "- The the model to ignore special tokens. In pytorch, index `-100` is ignored.\n",
    "- Find words that were splitted\n",
    "- Give all splitted parts of a word the same NER label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # tokenize\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        # same words have same word_id\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # if it isn't a word (), ignore\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 17751, 11639, 93376, 12026, 20575, 10114, 26905, 48426, 11160, 10109, 27012, 119, 102], [101, 10979, 46006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 3, 0, 0, 7, 0, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function returns a dict\n",
    "tokenize_and_align_labels(ds[\"train\"][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Btw, it's very easy to process a whole dataset with `.map()`:  \n",
    "The map function adds the new columns to all the splits of the existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_aligned = ds.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is ready.  \n",
    "\n",
    "Now, how do we evaluate the model?  \n",
    "[Seqeval](https://github.com/chakki-works/seqeval) is designed to evaluate NER labels, i.e. IOB notation\n",
    "\n",
    "We'll just load it and make some format conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': {'precision': np.float64(1.0),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1': np.float64(1.0),\n",
       "  'number': np.int64(1)},\n",
       " 'overall_precision': np.float64(1.0),\n",
       " 'overall_recall': np.float64(1.0),\n",
       " 'overall_f1': np.float64(1.0),\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how it works\n",
    "metric.compute(\n",
    "    predictions=[[\"O\", \"O\", \"B-PER\"]],\n",
    "    references=[[\"O\", \"O\", \"B-PER\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling data formats (optional):  \n",
    "We will need to do a bit of post-processing on our predictions.\n",
    "- select the predicted index (with the maximum logit) for each token\n",
    "- convert it to its string label\n",
    "- ignore everywhere we set a label of -100\n",
    "\n",
    "The following function does all this post-processing on the result of `Trainer.evaluate` \n",
    "(which is a namedtuple containing predictions and labels) before applying the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `transformers.Trainer`. To instantiate a `Trainer`, we will need to define three more things. The most important is the `TrainingArguments`, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional.\n",
    "\n",
    "For the full overview of possible training arguments, see [documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# \"data collator\" to take care of the padding & turncation problems\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ner\", # name of this new fine-tuned model\n",
    "    eval_strategy=\"epoch\", # evaluation at the end of each epoch, instead of none\n",
    "    learning_rate=2e-5, # slightly lower than default\n",
    "    num_train_epochs=3, # default\n",
    "    weight_decay=0.01, #regularization to prevent overfitting\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds_aligned[\"train\"],\n",
    "    eval_dataset=ds_aligned[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we haven't selected any columns?  \n",
    "That's because `DataCollator` automatically uses these columns (if it can find them):\n",
    "- `input_ids` and `attention_mask` for input \n",
    "- `labels` for output\n",
    "\n",
    "But let's train (will take ~3 min on a GPU, ~10 hrs on a CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 08:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.068274</td>\n",
       "      <td>0.915576</td>\n",
       "      <td>0.924828</td>\n",
       "      <td>0.920179</td>\n",
       "      <td>0.981419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.069891</td>\n",
       "      <td>0.939521</td>\n",
       "      <td>0.930116</td>\n",
       "      <td>0.934795</td>\n",
       "      <td>0.984285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.067315</td>\n",
       "      <td>0.935928</td>\n",
       "      <td>0.938049</td>\n",
       "      <td>0.936987</td>\n",
       "      <td>0.985001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.07295154547637035, metrics={'train_runtime': 488.2483, 'train_samples_per_second': 86.274, 'train_steps_per_second': 10.79, 'total_flos': 1327458291095034.0, 'train_loss': 0.07295154547637035, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/432 00:00 < 00:07, 57.83 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>MISC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "      <th>overall_precision</th>\n",
       "      <th>overall_recall</th>\n",
       "      <th>overall_f1</th>\n",
       "      <th>overall_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.917406</td>\n",
       "      <td>0.785657</td>\n",
       "      <td>0.895665</td>\n",
       "      <td>0.939371</td>\n",
       "      <td>0.899695</td>\n",
       "      <td>0.900315</td>\n",
       "      <td>0.900005</td>\n",
       "      <td>0.973204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.933982</td>\n",
       "      <td>0.773940</td>\n",
       "      <td>0.895397</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.899695</td>\n",
       "      <td>0.900315</td>\n",
       "      <td>0.900005</td>\n",
       "      <td>0.973204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.779755</td>\n",
       "      <td>0.895531</td>\n",
       "      <td>0.934942</td>\n",
       "      <td>0.899695</td>\n",
       "      <td>0.900315</td>\n",
       "      <td>0.900005</td>\n",
       "      <td>0.973204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>2878.000000</td>\n",
       "      <td>1274.000000</td>\n",
       "      <td>3346.000000</td>\n",
       "      <td>2664.000000</td>\n",
       "      <td>0.899695</td>\n",
       "      <td>0.900315</td>\n",
       "      <td>0.900005</td>\n",
       "      <td>0.973204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   LOC         MISC          ORG          PER  \\\n",
       "precision     0.917406     0.785657     0.895665     0.939371   \n",
       "recall        0.933982     0.773940     0.895397     0.930556   \n",
       "f1            0.925620     0.779755     0.895531     0.934942   \n",
       "number     2878.000000  1274.000000  3346.000000  2664.000000   \n",
       "\n",
       "           overall_precision  overall_recall  overall_f1  overall_accuracy  \n",
       "precision           0.899695        0.900315    0.900005          0.973204  \n",
       "recall              0.899695        0.900315    0.900005          0.973204  \n",
       "f1                  0.899695        0.900315    0.900005          0.973204  \n",
       "number              0.899695        0.900315    0.900005          0.973204  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's make a proper report per table\n",
    "predictions, labels, _ = trainer.predict(ds_aligned[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have successfully run the fine-tuning, your model should be good enough to win [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-LOC', 'O']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model we just trained\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-multilingual-cased-finetuned-ner\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"Franz Kafka studied at Charles University in Prague\", return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# which index is the maximum of the last label?\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "predicted_tokens_classes = [id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we got it right!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Probing\n",
    "\n",
    "With probing we are trying to answer the question:  \n",
    "Has the model simply learned the task, or has encoded genuine information?  \n",
    "\n",
    "There are many way how to approach it. Let's pick one:  \n",
    "- Does the model overrely on captial letters? How does it perform if you make everything uppercase or lowercase?\n",
    "- How does it generalize on a new NER test set dataset?\n",
    "- How does it generalize if we translate some of the examples to a different language?\n",
    "- Does it make any mistakes in [world-universities](https://github.com/endSly/world-universities-csv/blob/master/world-universities.csv)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model trained using the script above\n",
    "\n",
    "# use this tokenizer if you work on data which is \"like this\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"janko/distilbert-base-multilingual-cased-finetuned-ner\",\n",
    "    padding=True\n",
    "    )\n",
    "\n",
    "# use this one if you want to work on data which is [\"like\", \"this\"]\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"janko/distilbert-base-multilingual-cased-finetuned-ner\",\n",
    "#     padding=True, is_split_into_words=True\n",
    "#     )\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"janko/distilbert-base-multilingual-cased-finetuned-ner\",\n",
    "    output_hidden_states=True\n",
    "    )\n",
    "\n",
    "# Check if gpu is available, if not pick cpu\n",
    "# (cuda is the gpu driver: software that allows python and the gpu to talk)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize pipeline on the chosen device\n",
    "nlp = pipeline(\"ner\", tokenizer=tokenizer, model=model, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
