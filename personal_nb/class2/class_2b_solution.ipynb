{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification cont.\n",
    "\n",
    "Let's test the performance of our classification models from last week on the test data! I'm quickly going to run my best attempt at a model from last week and see how it performs on the test data. You can substitute in your best model from last week and see how it performs on the test data, or you can copy the last couple of cells from this week into last weeks notebook to see how it performs.\n",
    "\n",
    "### Install packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install asent\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asent import lexicons\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = pd.DataFrame(lexicons.get(\"lexicon_en_v1\").items(), columns=[\"word\", \"sentiment\"])\n",
    "\n",
    "train, test = train_test_split(lex, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarise the outcome variable\n",
    "y = [1 if x>0 else 0 for x in train[\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embeddings\n",
    "embeddings = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've swapped the way I handle out-of-vocabulary words to use the mean of the embeddings for the words in the training data, instead of using a zero vector. The mean embedding has the same shape as the individual embeddings, the difference is that every dimension in the mean embedding is the mean of the corresponding dimension in the individual embeddings. This is a more sophisticated way of handling out-of-vocabulary words, and it should improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_mean = np.mean([embeddings[r[\"word\"]] for i, r in train.iterrows() if r[\"word\"] in embeddings.index_to_key], axis=0)\n",
    "embeddings_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [embeddings[r[\"word\"]] if r[\"word\"] in embeddings.index_to_key else embeddings_mean for i, r in train.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "I've made a few tweeks to the model architecture to try and improve performance. First, I've added information about our class weight imbalance: we have more negative outcomes (0) than positive outcomes (1), so adding information about the imbalance should help the model adjust the weights according to the class distribution. Second, I've changed the algorithms used for optimisation (solver) to the liblinear solver, which is better suited to smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.count(0) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42,\n",
    "                         class_weight={0:0.55,1:0.45},\n",
    "                         solver=\"liblinear\",\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried running a random forest model, which seems to outperform the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_forest = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_forest.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "Now let's see how the model performs on the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1 if x>0 else 0 for x in test[\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note is that we still use the mean of the embeddings from the **training data** for out-of-vocabulary words in the test data. This is because we want the test set to represent out-of-sample data to test how well our model generalises to unseen data - if the test data had actually been unavailable to us when we trained the model, we wouldn't have been able to use the embeddings from the test data to handle out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = [embeddings[r[\"word\"]] if r[\"word\"] in embeddings.index_to_key else embeddings_mean for i, r in test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we don't fit the model to the data again, we just use the model we trained on the training data to predict the labels for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_forest.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
